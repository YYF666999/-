{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24237094-a8b7-4312-b471-543911d64939",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import jieba\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "BATCH_SIZE = 32\n",
    "# 选择数据集, 数据集包括 CR | MPQA | MR | SUBJ | SST\n",
    "DATA_SET = 'CR'\n",
    "\n",
    "train_path = \"../DataSet/\" + DATA_SET + \"/train.tsv\"\n",
    "test_path = \"../DataSet/\" + DATA_SET + \"/test.tsv\"\n",
    "word_vector_path = \"../sub_word_vector/\" + DATA_SET + \"_word_vector.txt\"\n",
    "\n",
    "print('Hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d3ca28-1f58-46c7-b032-9eba2ad8a15e",
   "metadata": {},
   "source": [
    "## 加载词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3c6ec-b6e7-4adf-ba53-eacc0589796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "word_dim = 100 # 词向量维度\n",
    "# 获取单词到索引的映射表以及每个单词的词向量表\n",
    "word_to_index = {'<unknown>': 1, '<padded>': 1}  # 根据筛选出来的词向量文件\"word_vector.txt\" 生成单词和索引的字典\n",
    "zero_ls = [0.0 for i in range(word_dim)]\n",
    "ls = [zero_ls, zero_ls]  # 用一个列表ls来存储词向量 前两个分别是 100 维的0向量，用来表示unknown_token和pad_token\n",
    "index_to_word = {}\n",
    "with open(word_vector_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        # 将形如 \"the -0.038194 -0.24487 0.72812 -0.39961...\"的字符串分成对应的词和词向量，比如\"the\"， \"-0.038194 -0.24487 0.72812 -0.39961...\"\n",
    "        #并且构建单词映射到整数的字典，word_to_index。 eg. 'the' : 2  ，PS：0和1分别是两个特殊字符串 unknown 和 padded的索引，the作为词向量文件中的第一个单词，所以下标为2\n",
    "        word_vector = line.split()\n",
    "        word_to_index[word_vector[0]] = i + 2  # 前两位由unknown和 padded分别占据\n",
    "        tmp = [] # 存储一个单词的词向量，总共是100个数字\n",
    "        for j, word in enumerate(word_vector):\n",
    "            if j == 0: #第一个是单词，所以跳过，只需要每个单词后面的词向量\n",
    "                continue\n",
    "            tmp.append(float(word))\n",
    "        ls.append(tmp) #每个单词的词向量又存到列表ls当中\n",
    "for key, value in word_to_index.items():\n",
    "    index_to_word[value] = key\n",
    "\n",
    "word_vector_weight_matrix = paddle.to_tensor(ls) #将词向量列表转换为Tensor\n",
    "VOCAB_SIZE = len(word_to_index) + 2\n",
    "# print(word_vector_weight_matrix.size())\n",
    "print(word_vector_weight_matrix.shape[0])\n",
    "print(word_vector_weight_matrix.shape[1])\n",
    "print(len(word_to_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0a135-ee57-4312-8111-efa3fa1fad68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ls = [[1, 2, 3], [2, 3, 4]]\n",
    "ls_to_tensor = paddle.to_tensor(ls)\n",
    "print(ls_to_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9590cf1a-99b9-4781-92a1-d022fe74c5cd",
   "metadata": {},
   "source": [
    "## 构造数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3038a73-302c-462b-87e4-0acb9abc6e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 获取语料中每个句子的长度，并且将句子分词，分词后的句子存放到列表中，句子和句子长度以元组的形式又存放在一个列表当中\n",
    "# return: eg.[([\"haha\"], 1), ([\"I\" \"love\" \"China\"], 3)]\n",
    "# path为存放语料的文件路径\n",
    "\n",
    "def get_sentences(path):\n",
    "    sentences = []\n",
    "    batch_sentences = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # 跳过第一行的text label\n",
    "            if i == 0:\n",
    "                continue\n",
    "            try:\n",
    "                words = list(jieba.cut(line.split('\\t')[0], cut_all=False))  # 需要将末尾的0或者1去掉\n",
    "                label = float(line.split('\\t')[1].strip())\n",
    "            except BaseException:\n",
    "                print(line.split('\\t'))\n",
    "            count = 0  # 统计每个句子的长度\n",
    "            for word in words:\n",
    "                count += 1\n",
    "            tmp = (words, count, label)\n",
    "            sentences.append(tmp)\n",
    "    return sentences\n",
    "\n",
    "# 根据上面得到的word_to_index,将单词转换为数字,列表形式为[ ([words_list1], len1, label1), ([words_list2], len2, label2),......], word_list是多个单词组成的序列\n",
    "def lookup_table(array_ls):\n",
    "    # 注意，array_ls是经过排序之后再传入到look_table()方法中的\n",
    "    sentences_tensor = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tensor_ls = []\n",
    "        # sentence[0]是一个包含多个单词的列表\n",
    "        for word in sentence[0]:\n",
    "            # word_to_index 是一个单词到索引的字典\n",
    "            if word in word_to_index.keys():\n",
    "                tensor_ls.append(word_to_index[word])  # 将单词转换为索引，并且索引存入张量当中\n",
    "            else:\n",
    "                tensor_ls.append(1)  # 如果在索引表中没找到单词，则“不认识”单词,用1下标代替, 此时unknown_token和padded_token下标都是1\n",
    "        sentences_tensor.append((tensor_ls, sentence[1], sentence[2]))\n",
    "    return sentences_tensor\n",
    "\n",
    "# 对句子进行填充，eg:16个句子组成一个batch,每个batch的句子长度必须相等，这里的做法是，获取batch中长度最长的句子,然后句子长度不够的append 1\n",
    "# 输入 [([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. [([1, 2, 3],3, 0), ([2, 3], 2, 1), ...]\n",
    "# 输出，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# eg. 假设batch_size = 2\n",
    "# [([1, 2, 3], 3), ([2, 3, 1], 2),...] 第二个句子列表 append 1，但是实际长度为2\n",
    "def pad_sentence_plus(array_ls, batch_size):\n",
    "    # 这里在不排序的情况下，每一个batch的数据进行一次填充\n",
    "    ans = 0 # j记录每个batch的第一条数据的下标\n",
    "    max = array_ls[0][1] # max为每个batch的句子最大长度\n",
    "    for i in range(len(array_ls)):\n",
    "        # 需要考虑最后一个batch可能长度不够batch_size\n",
    "        if (i + 1) % batch_size == 0 or i == len(array_ls)-1:\n",
    "            if array_ls[i][1]>max:\n",
    "                max = array_ls[i][1]\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                index = batch_size\n",
    "            else:\n",
    "                index = i - ans + 1\n",
    "            for j in range(index):\n",
    "                while(len(array_ls[j+ans][0])<max):\n",
    "                    array_ls[j+ans][0].append(1)\n",
    "            # 每一次填充完毕后，需要更新标记，并再次初始化最大值\n",
    "            ans = i + 1\n",
    "            if ans!=len(array_ls):\n",
    "                max = array_ls[ans][1]\n",
    "        else:\n",
    "            if array_ls[i][1]>max:\n",
    "                max = array_ls[i][1]\n",
    "    return array_ls\n",
    "\n",
    "# 输入，[([单词下标]，句子长度1, 标签1), ([单词下标]，句子长度2, 标签2), ....]\n",
    "# [([1, 2, 3],3, 1.0), ([2, 3, 1], 2, 0.0),...]\n",
    "# shuffle表示是否将每个batch打乱\n",
    "# batch_first if false 表示返回的 文本张量 形状为 (sentence_len,batch_size),\n",
    "# if true 表示返回的 文本张量 形状为(batch_size,sentence_len)\n",
    "# 这里默认采用batch_first = False,主要是为了适应nn.Embedding层的输入形状\n",
    "# 无论batch_first,  标签张量 形状都是一样的\n",
    "def iterator(array_ls, batch_size, shuffle=True, batch_first=False):\n",
    "    sentences_index = []  # 存放填充后的语句列表\n",
    "    tmp_sen = []\n",
    "    tmp_label = []\n",
    "    for i, sentence in enumerate(array_ls):\n",
    "        tmp_sen.append(sentence[0]) # 存放一个batch的数据\n",
    "        tmp_label.append(sentence[2]) # 存放一个batch的标签\n",
    "        if (i + 1) % batch_size == 0: #\n",
    "            sentences_index.append((tmp_sen, tmp_label))\n",
    "            tmp_sen = [] # 清空数据\n",
    "            tmp_label = [] # 清空标签\n",
    "    # 最后几个样本可能不够一个batch,需要额外判断\n",
    "    if len(tmp_sen) != 0:\n",
    "        sentences_index.append((tmp_sen, tmp_label))\n",
    "    if shuffle:\n",
    "        random.shuffle(sentences_index)  # 打乱列表中各个batch的顺序\n",
    "    res = []\n",
    "    # 2D张量转置\n",
    "    if batch_first == False:\n",
    "        for batch in sentences_index:\n",
    "            res.append((paddle.to_tensor(batch[0]).t(), paddle.to_tensor(batch[1])))\n",
    "    else:\n",
    "        for batch in sentences_index:\n",
    "            res.append((paddle.to_tensor(batch[0]), paddle.to_tensor(batch[1])))\n",
    "    return res\n",
    "\n",
    "def get_iterator(path, batch_size):\n",
    "    sentences = get_sentences(path)\n",
    "    lookup_sentences = lookup_table(sentences)\n",
    "    padded_sentences_plus = pad_sentence_plus(lookup_sentences, batch_size)\n",
    "    Iterator = iterator(padded_sentences_plus, batch_size)\n",
    "    return Iterator\n",
    "\n",
    "print(\"开始构造迭代器:\")\n",
    "train_iterator = get_iterator(train_path, BATCH_SIZE)\n",
    "test_iterator = get_iterator(test_path, BATCH_SIZE)\n",
    "# for batch in test_iterator:\n",
    "#     print(batch[1])\n",
    "# print(test_iterator)\n",
    "\n",
    "print(\"迭代器构造完毕，开始将数据投入到网络当中:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ef938-b8d0-4780-b9b9-d990839e7baf",
   "metadata": {},
   "source": [
    "## 投影测量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5224af3f-630c-4662-ac74-b324d785ed88",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class projection(nn.Layer):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(projection, self).__init__()\n",
    "        self.projector = paddle.standard_normal((2, Embedding_dim, 1))\n",
    "#         self.pad = nn.ZeroPad2d(1)\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        amplitude = inputs[0]\n",
    "        phase = inputs[1]\n",
    "        \n",
    "        amplitude_permute = paddle.transpose(amplitude, perm=[0, 1, 2])\n",
    "\n",
    "        amplitude_norm = paddle.zeros(shape=[amplitude_permute.shape[0], amplitude_permute.shape[1], amplitude_permute.shape[2]])\n",
    "    \n",
    "        amplitude_norm = F.normalize(amplitude_permute, 2, 2)\n",
    "        # paddle.transpose(ls_to_tensor, perm=[1, 0, 2])\n",
    "        phase_permute = paddle.transpose(phase, perm=[1, 0, 2])\n",
    "\n",
    "        real_part = amplitude_norm*paddle.cos(phase_permute)\n",
    "        imag_part = amplitude_norm*paddle.sin(phase_permute)\n",
    "\n",
    "        real_part_expand = paddle.unsqueeze(real_part, axis=3)\n",
    "        imag_part_expand = paddle.unsqueeze(imag_part, axis=3)\n",
    "       \n",
    "        real_part_expand_transpose = paddle.transpose(real_part_expand, perm=[0, 1, 3, 2])\n",
    "        imag_part_expand_transpose = paddle.transpose(imag_part_expand, perm=[0, 1, 3, 2])\n",
    "       \n",
    "        v_real = paddle.matmul(real_part_expand, real_part_expand_transpose) - paddle.matmul(imag_part_expand, imag_part_expand_transpose)\n",
    "        v_imag = paddle.matmul(imag_part_expand, real_part_expand_transpose) + paddle.matmul(real_part_expand, imag_part_expand_transpose)\n",
    "        \n",
    "        v_real_avg = paddle.zeros(shape=[v_real.shape[0], v_real.shape[2], v_real.shape[3]], dtype='float32')\n",
    "        v_imag_avg = paddle.zeros(shape=[v_imag.shape[0], v_imag.shape[2], v_imag.shape[3]], dtype='float32')\n",
    "\n",
    "        v_real_avg = paddle.mean(v_real, axis=1, keepdim=False)\n",
    "        v_imag_avg = paddle.mean(v_imag, axis=1, keepdim=False)\n",
    "        # v_real_avg (batch_size, embedding_dim, embedding_dim)\n",
    "     \n",
    "        p_real = self.projector[0]\n",
    "        p_imag = self.projector[1]\n",
    "\n",
    "        p_real_norm = p_real / paddle.norm(p_real, axis=0)\n",
    "        p_imag_norm = p_imag / paddle.norm(p_imag, axis=0)\n",
    "        \n",
    "        p_real_mat = paddle.matmul(p_real_norm, paddle.transpose(p_real_norm, perm=[1, 0]))\n",
    "        p_imag_mat = paddle.matmul(p_imag_norm, paddle.transpose(p_imag_norm, perm=[1, 0]))\n",
    "\n",
    "        Pv_real = paddle.matmul(v_real_avg, p_real_mat) - paddle.matmul(v_imag_avg, p_imag_mat)\n",
    "        Pv_imag = paddle.matmul(v_real_avg, p_imag_mat) + paddle.matmul(v_imag_avg, p_real_mat)\n",
    "\n",
    "        return [paddle.unsqueeze(Pv_real, axis=1), paddle.unsqueeze(Pv_imag, axis=1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2349a6a0-3ca1-4428-b4b5-44f45b75ce19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 映射成三个不同的向量的自注意力机制\n",
    "class self_attention(nn.Layer):\n",
    "    def __init__(self, Embedding_dim):\n",
    "        super(self_attention, self).__init__()\n",
    "        self.mapping_query = paddle.standard_normal((Embedding_dim, Embedding_dim))\n",
    "        self.mapping_key = paddle.standard_normal((Embedding_dim, Embedding_dim))\n",
    "        self.mapping_value = paddle.standard_normal((Embedding_dim, Embedding_dim))\n",
    "    def forward(self, inputs):\n",
    "        query = paddle.matmul(paddle.transpose(inputs, perm=[1, 0, 2]), self.mapping_query)\n",
    "        key = paddle.matmul(paddle.transpose(inputs, perm=[1, 0, 2]), self.mapping_key)\n",
    "        value = paddle.matmul(paddle.transpose(inputs, perm=[1, 0, 2]), self.mapping_value)\n",
    "        query = query / paddle.reshape(paddle.norm(query, axis=2), shape=[query.shape[0], query.shape[1], 1])\n",
    "        key = key / paddle.reshape(paddle.norm(key, axis=2), shape=[key.shape[0], key.shape[1], 1])\n",
    "        value = value / paddle.reshape(paddle.norm(value, axis=2),shape=[value.shape[0], value.shape[1], 1])\n",
    "\n",
    "        return paddle.matmul(F.softmax((paddle.matmul(query, paddle.transpose(key, perm=[0, 2, 1])))/np.sqrt(inputs.shape[1]), axis=2), value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e797173-5c58-4002-a3d7-992986616305",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45dd59-06e7-4334-989e-6f98bd616efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Pretrained_Attr = paddle.ParamAttr(name='amplitude_embedding',\n",
    "                                   initializer=paddle.nn.initializer.Assign(word_vector_weight_matrix),\n",
    "                                   trainable=True)\n",
    "class CICWEQNN(nn.Layer):\n",
    "    def __init__(self, weight_matrix, embedding_dim, hidden_dim, output_dim, pad_idx, pretrained_attr):\n",
    "        super(CICWEQNN, self).__init__()\n",
    "        \n",
    "        self.amplitude_embedding = paddle.nn.Embedding(num_embeddings=word_vector_weight_matrix.shape[0],\n",
    "                                      embedding_dim=word_vector_weight_matrix.shape[1],\n",
    "                                      padding_idx=word_to_index['<padded>'],\n",
    "                                      weight_attr=pretrained_attr)\n",
    "        self.phase_embedding = nn.Embedding(VOCAB_SIZE, 1, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(embedding_dim, embedding_dim)\n",
    "        self.Conv2dOne = nn.Conv2D(1, 1, 3)\n",
    "        self.Conv2dTwo = nn.Conv2D(1, 1, 3)\n",
    "\n",
    "        self.MaxPool1 = nn.MaxPool2D((embedding_dim-2, 1), 1)\n",
    "        self.MaxPool2 = nn.MaxPool2D((embedding_dim-2, 1), 1)\n",
    "        self.attention = self_attention(embedding_dim)\n",
    "        self.projection_measurement = projection(embedding_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(2*(embedding_dim-2), 10) \n",
    "        self.fc2 = nn.Linear(10, output_dim)\n",
    "    def forward(self, text):\n",
    "       \n",
    "        amplitude = self.amplitude_embedding(text)\n",
    "        \n",
    "        phase = self.phase_embedding(text)\n",
    "        \n",
    "        amplitude_plus = self.gru(amplitude)\n",
    "        \n",
    "        amplitude_plus2 = self.attention(amplitude_plus[0])\n",
    "        \n",
    "        \n",
    "        embedded = [amplitude_plus2, phase]\n",
    "        \n",
    "        project = self.projection_measurement(embedded)\n",
    "        \n",
    "        Conv_real = self.Conv2dOne(project[0])\n",
    "        \n",
    "        Conv_imag = self.Conv2dTwo(project[1])\n",
    "        \n",
    "        Max_real = self.MaxPool1(nn.Sigmoid()(Conv_real))\n",
    "        \n",
    "        Max_imag = self.MaxPool2(nn.Sigmoid()(Conv_imag))\n",
    "        \n",
    "        fc1 = self.fc1(paddle.concat(x=[Max_real, Max_imag],axis=3))\n",
    "        \n",
    "        return nn.Sigmoid()(self.fc2(nn.Sigmoid()(fc1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0926cac9-3ce9-442c-8565-d83d70d84b8b",
   "metadata": {},
   "source": [
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d53af-2064-4733-849d-2ec2107f19ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "PAD_IDX = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688cd0f-7fcd-41c5-bfb2-336257ebb3e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34ed34-5250-45fd-a27f-368dea38ecfd",
   "metadata": {},
   "source": [
    "## 模型训练和预测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67db8aa1-e0c6-47fc-99a4-63688dfc682d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# dataset与mnist的定义与第一部分内容一致\n",
    "\n",
    "# 用 DataLoader 实现数据加载\n",
    "#train_loader = paddle.io.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = paddle.round(preds)\n",
    "    \n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y) #convert into float for division\n",
    "    return correct.sum() / len(correct)\n",
    "    \n",
    "model=CICWEQNN(word_vector_weight_matrix,\n",
    "            EMBEDDING_DIM,\n",
    "            HIDDEN_DIM,\n",
    "            OUTPUT_DIM,\n",
    "            PAD_IDX, Pretrained_Attr)\n",
    "\n",
    "\n",
    "# 设置迭代次数\n",
    "epochs = 50\n",
    "\n",
    "# 设置优化器\n",
    "optim = paddle.optimizer.Adam(parameters=model.parameters())\n",
    "# 设置损失函数\n",
    "loss_fn = nn.BCELoss()\n",
    "# 训练\n",
    "def train(model, train_iterator):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch_id, data in enumerate(train_iterator):\n",
    "\n",
    "        x_data = data[0]           # 训练数据\n",
    "        \n",
    "        \n",
    "        y_data = data[1]    # 训练数据标签\n",
    "        \n",
    "        predicts = paddle.squeeze(model(x_data))    # 预测结果\n",
    "        \n",
    "        # 计算损失 等价于 prepare 中loss的设置\n",
    "\n",
    "        loss = loss_fn(predicts, y_data)\n",
    "        \n",
    "        # 计算准确率 等价于 prepare 中metrics的设置\n",
    "        acc = binary_accuracy(predicts, y_data)\n",
    "        \n",
    "        epoch_loss += loss.numpy().item()\n",
    "        \n",
    "        epoch_acc += acc.numpy().item()\n",
    "        \n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 更新参数\n",
    "        optim.step()\n",
    "\n",
    "        # 梯度清零\n",
    "        optim.clear_grad()\n",
    "    return epoch_loss / len(train_iterator), epoch_acc / len(train_iterator)\n",
    "# 预测\n",
    "def test(model, test_iterator):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    model.eval()\n",
    "    for batch_id, data in enumerate(test_iterator):\n",
    "        x_data = data[0]            # 测试数据\n",
    "    \n",
    "        y_data = data[1]     # 测试数据标签\n",
    "        predicts = paddle.squeeze(model(x_data))    # 预测结果\n",
    "        # 计算损失 \n",
    "        loss = loss_fn(predicts, y_data)\n",
    "        # 计算准确率 \n",
    "        acc = binary_accuracy(predicts, y_data)\n",
    "        epoch_loss += loss.numpy().item()\n",
    "        \n",
    "        epoch_acc += acc.numpy().item()\n",
    "\n",
    "    return epoch_loss / len(test_iterator), epoch_acc / len(test_iterator)\n",
    "\n",
    "best_acc = -1\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator)\n",
    "    test_loss, test_acc = test(model, test_iterator)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if(test_acc > best_acc):\n",
    "        best_acc = test_acc\n",
    "    \n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {round(train_loss,3)} | Train Acc: {round(train_acc * 100, 2)}% ')\n",
    "    print(f'\\t test. Loss: {round(test_loss,3)} |  test. Acc: {round(test_acc * 100, 2)}% |  best_acc: {round(best_acc * 100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672ab9c0-bca4-4212-9ab8-9fbb25451f86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(paddle.zeros(shape=[3, 2], dtype='float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce1ae77-30c3-497d-bd3a-9f76e7a6f69f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#v_real_avg = torch.mean(v_real, dim=1, keepdim=False)\n",
    " #       v_imag_avg = torch.mean(v_imag, dim=1, keepdim=False)\n",
    "\n",
    "\n",
    "key = paddle.to_tensor([[[1, 2, 3], [4, 5, 6]],[[7, 8, 9], [10, 11, 12]]])\n",
    "key_trans = paddle.transpose(key, perm=[0, 2, 1])\n",
    "print(key_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cb1c2-c0e6-4bb6-8cb9-bf17ba39dbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = paddle.round(preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y) #convert into float for division\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "    \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "a = paddle.to_tensor([0.6, 0.2, 0.2, 0.1])\n",
    "b = paddle.to_tensor([1, 1, 0, 0])\n",
    "binary_accuracy(a, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb382f-2a0a-4f49-a265-5e2275ade971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = paddle.round(preds)\n",
    "    print(rounded_preds)\n",
    "#     print(f'preds:{preds}, y:{y}')\n",
    "    correct = (rounded_preds == y) #convert into float for division\n",
    "    return correct.sum() / len(correct)\n",
    "    # print('batch: 模型预测值', rounded_preds,'真实标签', y)\n",
    "a = paddle.to_tensor(\n",
    "       [0.50367379, 0.50367212, 0.50367010, 0.50367481, 0.50366598, 0.50366402,\n",
    "        0.50366652, 0.50366569, 0.50366539, 0.50366944, 0.50366831, 0.50366378,\n",
    "        0.50366819, 0.50366312, 0.50366414, 0.50366485])\n",
    "b = paddle.to_tensor(\n",
    "       [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.])\n",
    "\n",
    "print(binary_accuracy(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5898d251-c936-4a95-aaf2-45cc325d0d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = 'weibo_senti_100k'\n",
    "yourSet = 'dev.tsv'\n",
    "yourSetTmp = 'devTmp.tsv'\n",
    "with open('../DataSet/'+ dataset +'/'+ yourSetTmp, 'w', encoding='utf-8') as f1:\n",
    "    with open('../DataSet/'+ dataset +'/'+ yourSet, 'r', encoding='utf-8') as f2:\n",
    "        for i, line in enumerate(f2):\n",
    "            \n",
    "            if i != 0:\n",
    "                lineTmp = ''\n",
    "                lineTmp += (line[:-2] + ('0.0' if line[-2] == '0' else '1.0') + '\\n')\n",
    "                f1.write(lineTmp)\n",
    "            else:\n",
    "                f1.write(line)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2672342-a3d7-4e45-ab25-5429b6030aa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
